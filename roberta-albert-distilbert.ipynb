{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"405UjRvLGpHo","executionInfo":{"status":"ok","timestamp":1638680784379,"user_tz":300,"elapsed":9991,"user":{"displayName":"Anupam Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05817730190135440759"}},"outputId":"291d4305-6ea5-4b28-b654-48ac00716a3f","execution":{"iopub.status.busy":"2021-12-06T03:40:47.021763Z","iopub.execute_input":"2021-12-06T03:40:47.022026Z","iopub.status.idle":"2021-12-06T03:40:55.880013Z","shell.execute_reply.started":"2021-12-06T03:40:47.021995Z","shell.execute_reply":"2021-12-06T03:40:55.879222Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.12.5)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.8.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.3.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport transformers\nfrom transformers import BertConfig, TFBertForSequenceClassification, BertTokenizer, XLNetConfig, TFXLNetForSequenceClassification, XLNetTokenizer, XLMConfig, TFXLMForSequenceClassification, XLMTokenizer, RobertaConfig, TFRobertaForSequenceClassification, RobertaTokenizer, DistilBertConfig, TFDistilBertForSequenceClassification, DistilBertTokenizer, AlbertConfig, TFAlbertForSequenceClassification, AlbertTokenizer\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\n\nprint('Transformers version: ', transformers.__version__)\nprint('Tensorflow version: ', tf.__version__)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"fdSO7nrtGbpJ","executionInfo":{"status":"ok","timestamp":1638681050821,"user_tz":300,"elapsed":8985,"user":{"displayName":"Anupam Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05817730190135440759"}},"outputId":"3e823fe9-06ae-4e3d-e6ea-343f695fe86d","execution":{"iopub.status.busy":"2021-12-06T03:40:55.881948Z","iopub.execute_input":"2021-12-06T03:40:55.882249Z","iopub.status.idle":"2021-12-06T03:41:03.331790Z","shell.execute_reply.started":"2021-12-06T03:40:55.882197Z","shell.execute_reply":"2021-12-06T03:41:03.331042Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Transformers version:  4.12.5\nTensorflow version:  2.6.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import Data","metadata":{"id":"RyZhHl4BGbpQ"}},{"cell_type":"code","source":"try:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    in_colab = True\nexcept:\n    in_colab = False\n    \nif os.environ.get('KAGGLE_KERNEL_RUN_TYPE', ''):\n    env = 'Kaggle'\n\nif in_colab:\n    data_dir = '/content/drive/MyDrive/NLP Project/codes/input'\nelif env == 'Kaggle':\n    data_dir = '../input/nlp-getting-started/'\nelse:\n    data_dir = ''\ntrain_df = pd.read_csv(data_dir + '/train.csv')\ntest_df = pd.read_csv(data_dir + '/test.csv')","metadata":{"id":"kN-JodUhG1zd","executionInfo":{"status":"ok","timestamp":1638681145182,"user_tz":300,"elapsed":94392,"user":{"displayName":"Anupam Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05817730190135440759"}},"outputId":"5e06efb0-eb78-4966-c444-889379c76567","execution":{"iopub.status.busy":"2021-12-06T03:41:03.333093Z","iopub.execute_input":"2021-12-06T03:41:03.333378Z","iopub.status.idle":"2021-12-06T03:41:03.441533Z","shell.execute_reply.started":"2021-12-06T03:41:03.333343Z","shell.execute_reply":"2021-12-06T03:41:03.440779Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.sample(n=len(train_df), random_state=42)\n# sample_submission = pd.read_csv(data_dir+'sample_submission.csv')\nprint(train_df['target'].value_counts())\ntrain_df.head(2)","metadata":{"id":"5R1SnEQ0IiOb","executionInfo":{"status":"ok","timestamp":1638681210116,"user_tz":300,"elapsed":134,"user":{"displayName":"Anupam Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05817730190135440759"}},"outputId":"db0ba4ca-fbb5-4198-f35a-3a9f1d224c83","execution":{"iopub.status.busy":"2021-12-06T03:41:03.443720Z","iopub.execute_input":"2021-12-06T03:41:03.443974Z","iopub.status.idle":"2021-12-06T03:41:03.475013Z","shell.execute_reply.started":"2021-12-06T03:41:03.443941Z","shell.execute_reply":"2021-12-06T03:41:03.474154Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"0    4342\n1    3271\nName: target, dtype: int64\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"        id      keyword location  \\\n2644  3796  destruction      NaN   \n2227  3185       deluge      NaN   \n\n                                                   text  target  \n2644  So you have a new weapon that can cause un-ima...       1  \n2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2644</th>\n      <td>3796</td>\n      <td>destruction</td>\n      <td>NaN</td>\n      <td>So you have a new weapon that can cause un-ima...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2227</th>\n      <td>3185</td>\n      <td>deluge</td>\n      <td>NaN</td>\n      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Prep Functions","metadata":{"id":"UhHo6pWUGbpZ"}},{"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntree_tokenizer = TreebankWordTokenizer()\ndef get_tree_tokens(x):\n    x = tree_tokenizer.tokenize(x)\n    x = ' '.join(x)\n    return x\ntrain_df.text = train_df.text.apply(get_tree_tokens)\ntest_df.text = test_df.text.apply(get_tree_tokens)","metadata":{"id":"9NsGfISCGbpb","executionInfo":{"status":"ok","timestamp":1638681214768,"user_tz":300,"elapsed":2146,"user":{"displayName":"Anupam Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05817730190135440759"}},"execution":{"iopub.status.busy":"2021-12-06T03:41:03.476240Z","iopub.execute_input":"2021-12-06T03:41:03.476971Z","iopub.status.idle":"2021-12-06T03:41:06.088061Z","shell.execute_reply.started":"2021-12-06T03:41:03.476929Z","shell.execute_reply":"2021-12-06T03:41:06.087231Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# from: https://www.kaggle.com/utsavnandi/roberta-using-huggingface-tf-implementation\ndef to_tokens(input_text, tokenizer):\n    output = tokenizer.encode_plus(input_text, max_length=90, pad_to_max_length=True)\n    return output\n\ndef select_field(features, field):\n    return [feature[field] for feature in features]\n\nimport re\ndef clean_tweet(tweet):\n    # Removing the @\n    #tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n    # Removing the URL links\n    #tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n    # Keeping only letters\n    #tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n    # Removing additional whitespaces\n    tweet = re.sub(r\" +\", ' ', tweet)\n    return tweet\n\ndef preprocess_data(tokenizer, train_df, test_df):\n    train_text = train_df['text'].apply(clean_tweet)\n    test_text = test_df['text'].apply(clean_tweet)\n    train_encoded = train_text.apply(lambda x: to_tokens(x, tokenizer))\n    test_encoded = test_text.apply(lambda x: to_tokens(x, tokenizer))\n\n    #create attention masks\n    input_ids_train = np.array(select_field(train_encoded, 'input_ids'))\n    attention_masks_train = np.array(select_field(train_encoded, 'attention_mask'))\n\n    input_ids_test = np.array(select_field(test_encoded, 'input_ids'))\n    attention_masks_test = np.array(select_field(test_encoded, 'attention_mask'))\n\n    # concatonate masks\n    train_X = [input_ids_train, attention_masks_train]\n    test_X = [input_ids_test, attention_masks_test]\n    #OHE target\n    train_y = tf.keras.utils.to_categorical(train_df['target'].values.reshape(-1, 1))\n\n    return train_X, train_y, test_X","metadata":{"id":"HLkZBTHbGbpq","executionInfo":{"status":"ok","timestamp":1638681218393,"user_tz":300,"elapsed":134,"user":{"displayName":"Anupam Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05817730190135440759"}},"execution":{"iopub.status.busy":"2021-12-06T03:41:06.089716Z","iopub.execute_input":"2021-12-06T03:41:06.090001Z","iopub.status.idle":"2021-12-06T03:41:06.100033Z","shell.execute_reply.started":"2021-12-06T03:41:06.089966Z","shell.execute_reply":"2021-12-06T03:41:06.099023Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Function to load models","metadata":{"id":"-rVXJ5c4Gbpu"}},{"cell_type":"code","source":"# code from https://github.com/huggingface/transformers\n# Transformers has a unified API\n# for 10 transformer architectures and 30 pretrained weights.\n#          Model          | Tokenizer          | Pretrained weights shortcut\ndef load_pretrained_model(model_class='bert', model_name='bert-base-cased', task='binary', learning_rate=3e-5, epsilon=1e-8, lower_case=False):\n    MODEL_CLASSES = {\n      \"bert\": (BertConfig, TFBertForSequenceClassification, BertTokenizer),\n      \"xlnet\": (XLNetConfig, TFXLNetForSequenceClassification, XLNetTokenizer),\n      \"xlm\": (XLMConfig, TFXLMForSequenceClassification, XLMTokenizer),\n      \"roberta\": (RobertaConfig, TFRobertaForSequenceClassification, RobertaTokenizer),\n      \"distilbert\": (DistilBertConfig, TFDistilBertForSequenceClassification, DistilBertTokenizer),\n      \"albert\": (AlbertConfig, TFAlbertForSequenceClassification, AlbertTokenizer),\n      #\"xlmroberta\": (XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer), No tensorflow version yet\n    }\n    model_metrics = [\n          tf.keras.metrics.TruePositives(name='tp'),\n          tf.keras.metrics.FalsePositives(name='fp'),\n          tf.keras.metrics.TrueNegatives(name='tn'),\n          tf.keras.metrics.FalseNegatives(name='fn'), \n          tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n          tf.keras.metrics.Precision(name='precision'),\n          tf.keras.metrics.Recall(name='recall'),\n          tf.keras.metrics.AUC(name='auc'),\n    ]\n  \n    \n    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_class]\n  \n    config = config_class.from_pretrained(model_name, num_labels=2, finetuning_task=task)\n  \n  \n    model = model_class.from_pretrained(model_name)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon, clipnorm=1.0)\n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n    #model.summary()\n  \n    tokenizer = tokenizer_class.from_pretrained(model_name, lower_case = lower_case)\n  \n    return config, model, tokenizer","metadata":{"id":"LTItG6bbGbpx","executionInfo":{"status":"ok","timestamp":1638681222445,"user_tz":300,"elapsed":148,"user":{"displayName":"Anupam Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05817730190135440759"}},"execution":{"iopub.status.busy":"2021-12-06T03:41:06.101701Z","iopub.execute_input":"2021-12-06T03:41:06.101958Z","iopub.status.idle":"2021-12-06T03:41:06.117105Z","shell.execute_reply.started":"2021-12-06T03:41:06.101925Z","shell.execute_reply":"2021-12-06T03:41:06.116271Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{"id":"6h8zLSG-Gbpz"}},{"cell_type":"code","source":"# load model, process data for model\n_, _, tokenizer = load_pretrained_model(model_class='roberta', model_name='roberta-base', learning_rate=2e-5, lower_case=False)\ntrain_X, train_y, test_X = preprocess_data(tokenizer=tokenizer, train_df=train_df, test_df=test_df)\n\n\nkf = KFold(n_splits=6)\ntest_preds = []\ni = 0\nfor train_idx, test_idx in kf.split(train_X[0]):\n    i+=1\n    if i not in [1, 5]: #only do 2 folds to save time\n        continue\n    train_split_X = [train_X[i][train_idx] for i in range(len(train_X))]\n    test_split_X = [train_X[i][test_idx] for i in range(len(train_X))]\n\n    train_split_y = train_y[train_idx]\n    test_split_y = train_y[test_idx]\n    #create class weights to account for inbalance\n    positive = train_df.iloc[train_idx, :].target.value_counts()[0]\n    negative = train_df.iloc[train_idx, :].target.value_counts()[1]\n    pos_weight = positive / (positive + negative)\n    neg_weight = negative / (positive + negative)\n\n    # class_weight = [{0:pos_weight, 1:neg_weight}, {0:neg_weight, 1:pos_weight}]\n\n    K.clear_session()\n    config, model, tokenizer = load_pretrained_model(model_class='roberta', model_name='roberta-base', learning_rate=2e-5, lower_case=False)\n\n    # fit, test model\n    model.fit(train_split_X, train_split_y, batch_size=64, epochs=3, validation_data=(test_split_X, test_split_y))\n\n    val_preds = model.predict(test_split_X, batch_size=32, verbose=1)\n    val_preds = np.argmax(val_preds.logits, axis=1).flatten()\n    print('accuracy: ', metrics.accuracy_score(train_df.iloc[test_idx, :].target.values, val_preds))\n    print('f1 score: ', metrics.f1_score(train_df.iloc[test_idx, :].target.values, val_preds))\n\n    preds1 = model.predict(test_X, batch_size=32, verbose=1)\n    test_preds.append(preds1)","metadata":{"id":"vXHYyBAwGbpz","executionInfo":{"status":"error","timestamp":1638681590334,"user_tz":300,"elapsed":312163,"user":{"displayName":"Anupam Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05817730190135440759"}},"outputId":"dbb46e8f-aa0a-466c-b55a-c41c833193e3","execution":{"iopub.status.busy":"2021-12-06T04:31:03.740471Z","iopub.execute_input":"2021-12-06T04:31:03.740764Z","iopub.status.idle":"2021-12-06T04:41:28.098767Z","shell.execute_reply.started":"2021-12-06T04:31:03.740733Z","shell.execute_reply":"2021-12-06T04:41:28.097709Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n\nSome layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nAll model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n\nSome layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n100/100 [==============================] - 90s 717ms/step - loss: 0.4811 - accuracy: 0.7711 - val_loss: 0.3951 - val_accuracy: 0.8314\nEpoch 2/3\n100/100 [==============================] - 69s 692ms/step - loss: 0.3676 - accuracy: 0.8509 - val_loss: 0.3758 - val_accuracy: 0.8416\nEpoch 3/3\n100/100 [==============================] - 69s 693ms/step - loss: 0.3187 - accuracy: 0.8725 - val_loss: 0.3926 - val_accuracy: 0.8511\n40/40 [==============================] - 7s 128ms/step\naccuracy:  0.851063829787234\nf1 score:  0.82186616399623\n102/102 [==============================] - 13s 132ms/step\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n\nSome layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n100/100 [==============================] - 90s 718ms/step - loss: 0.4778 - accuracy: 0.7714 - val_loss: 0.4212 - val_accuracy: 0.8258\nEpoch 2/3\n100/100 [==============================] - 69s 692ms/step - loss: 0.3631 - accuracy: 0.8588 - val_loss: 0.4091 - val_accuracy: 0.8219\nEpoch 3/3\n100/100 [==============================] - 69s 692ms/step - loss: 0.3056 - accuracy: 0.8832 - val_loss: 0.4344 - val_accuracy: 0.8227\n40/40 [==============================] - 9s 127ms/step\naccuracy:  0.8226950354609929\nf1 score:  0.7867298578199051\n102/102 [==============================] - 13s 129ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# load model, process data for model\n_, _, tokenizer = load_pretrained_model(model_class='albert', model_name='albert-base-v2', learning_rate=2e-5, lower_case=False)\ntrain_X, train_y, test_X = preprocess_data(tokenizer=tokenizer, train_df=train_df, test_df=test_df)\n\n\nkf = KFold(n_splits=6)\ntest_preds = []\ni = 0\nfor train_idx, test_idx in kf.split(train_X[0]):\n    i+=1\n    if i not in [1, 5]: #only do 2 folds to save time\n        continue\n    train_split_X = [train_X[i][train_idx] for i in range(len(train_X))]\n    test_split_X = [train_X[i][test_idx] for i in range(len(train_X))]\n\n    train_split_y = train_y[train_idx]\n    test_split_y = train_y[test_idx]\n    #create class weights to account for inbalance\n    positive = train_df.iloc[train_idx, :].target.value_counts()[0]\n    negative = train_df.iloc[train_idx, :].target.value_counts()[1]\n    pos_weight = positive / (positive + negative)\n    neg_weight = negative / (positive + negative)\n\n    # class_weight = [{0:pos_weight, 1:neg_weight}, {0:neg_weight, 1:pos_weight}]\n\n    K.clear_session()\n    config, model, tokenizer = load_pretrained_model(model_class='albert', model_name='albert-base-v2', learning_rate=2e-5, lower_case=False)\n\n    # fit, test model\n    model.fit(train_split_X, train_split_y, batch_size=64, epochs=3, validation_data=(test_split_X, test_split_y))\n\n    val_preds = model.predict(test_split_X, batch_size=32, verbose=1)\n    val_preds = np.argmax(val_preds.logits, axis=1).flatten()\n    print('accuracy: ', metrics.accuracy_score(train_df.iloc[test_idx, :].target.values, val_preds))\n    print('f1 score: ', metrics.f1_score(train_df.iloc[test_idx, :].target.values, val_preds))\n\n    preds1 = model.predict(test_X, batch_size=32, verbose=1)\n    test_preds.append(preds1)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T04:41:28.100725Z","iopub.execute_input":"2021-12-06T04:41:28.100973Z","iopub.status.idle":"2021-12-06T04:51:40.320877Z","shell.execute_reply.started":"2021-12-06T04:41:28.100943Z","shell.execute_reply":"2021-12-06T04:51:40.319780Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"All model checkpoint layers were used when initializing TFAlbertForSequenceClassification.\n\nSome layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nAll model checkpoint layers were used when initializing TFAlbertForSequenceClassification.\n\nSome layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n100/100 [==============================] - 82s 708ms/step - loss: 0.4885 - accuracy: 0.7823 - val_loss: 0.4213 - val_accuracy: 0.8274\nEpoch 2/3\n100/100 [==============================] - 67s 672ms/step - loss: 0.3679 - accuracy: 0.8517 - val_loss: 0.4388 - val_accuracy: 0.8243\nEpoch 3/3\n100/100 [==============================] - 67s 672ms/step - loss: 0.3024 - accuracy: 0.8846 - val_loss: 0.4457 - val_accuracy: 0.8140\n40/40 [==============================] - 8s 132ms/step\naccuracy:  0.814026792750197\nf1 score:  0.7521008403361344\n102/102 [==============================] - 14s 135ms/step\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFAlbertForSequenceClassification.\n\nSome layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n100/100 [==============================] - 81s 695ms/step - loss: 0.4828 - accuracy: 0.7965 - val_loss: 0.4542 - val_accuracy: 0.8054\nEpoch 2/3\n100/100 [==============================] - 67s 672ms/step - loss: 0.3945 - accuracy: 0.8421 - val_loss: 0.4210 - val_accuracy: 0.8188\nEpoch 3/3\n100/100 [==============================] - 67s 672ms/step - loss: 0.3397 - accuracy: 0.8687 - val_loss: 0.4347 - val_accuracy: 0.8172\n40/40 [==============================] - 9s 136ms/step\naccuracy:  0.8171788810086682\nf1 score:  0.7531914893617021\n102/102 [==============================] - 14s 136ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# load model, process data for model\n_, _, tokenizer = load_pretrained_model(model_class='distilbert', model_name='distilbert-base-uncased', learning_rate=2e-5, lower_case=False)\ntrain_X, train_y, test_X = preprocess_data(tokenizer=tokenizer, train_df=train_df, test_df=test_df)\n\n\nkf = KFold(n_splits=6)\ntest_preds = []\ni = 0\nfor train_idx, test_idx in kf.split(train_X[0]):\n    i+=1\n    if i not in [1, 5]: #only do 2 folds to save time\n        continue\n    train_split_X = [train_X[i][train_idx] for i in range(len(train_X))]\n    test_split_X = [train_X[i][test_idx] for i in range(len(train_X))]\n\n    train_split_y = train_y[train_idx]\n    test_split_y = train_y[test_idx]\n    #create class weights to account for inbalance\n    positive = train_df.iloc[train_idx, :].target.value_counts()[0]\n    negative = train_df.iloc[train_idx, :].target.value_counts()[1]\n    pos_weight = positive / (positive + negative)\n    neg_weight = negative / (positive + negative)\n\n    # class_weight = [{0:pos_weight, 1:neg_weight}, {0:neg_weight, 1:pos_weight}]\n\n    K.clear_session()\n    config, model, tokenizer = load_pretrained_model(model_class='distilbert', model_name='distilbert-base-uncased', learning_rate=2e-5, lower_case=False)\n\n    # fit, test model\n    model.fit(train_split_X, train_split_y, batch_size=64, epochs=3, validation_data=(test_split_X, test_split_y))\n\n    val_preds = model.predict(test_split_X, batch_size=32, verbose=1)\n    val_preds = np.argmax(val_preds.logits, axis=1).flatten()\n    print('accuracy: ', metrics.accuracy_score(train_df.iloc[test_idx, :].target.values, val_preds))\n    print('f1 score: ', metrics.f1_score(train_df.iloc[test_idx, :].target.values, val_preds))\n\n    preds1 = model.predict(test_X, batch_size=32, verbose=1)\n    test_preds.append(preds1)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T04:51:40.322346Z","iopub.execute_input":"2021-12-06T04:51:40.322622Z","iopub.status.idle":"2021-12-06T04:56:50.788253Z","shell.execute_reply.started":"2021-12-06T04:51:40.322584Z","shell.execute_reply":"2021-12-06T04:56:50.787453Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_layer_norm', 'vocab_transform']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_24', 'classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSome layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_layer_norm', 'vocab_transform']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'pre_classifier', 'classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n100/100 [==============================] - 46s 366ms/step - loss: 0.4891 - accuracy: 0.7828 - val_loss: 0.4203 - val_accuracy: 0.8306\nEpoch 2/3\n100/100 [==============================] - 35s 347ms/step - loss: 0.3569 - accuracy: 0.8627 - val_loss: 0.4154 - val_accuracy: 0.8314\nEpoch 3/3\n100/100 [==============================] - 35s 347ms/step - loss: 0.2947 - accuracy: 0.8922 - val_loss: 0.4056 - val_accuracy: 0.8306\n40/40 [==============================] - 4s 66ms/step\naccuracy:  0.830575256107171\nf1 score:  0.7930702598652551\n102/102 [==============================] - 7s 64ms/step\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_layer_norm', 'vocab_transform']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'pre_classifier', 'classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n100/100 [==============================] - 46s 361ms/step - loss: 0.4826 - accuracy: 0.8017 - val_loss: 0.4296 - val_accuracy: 0.8172\nEpoch 2/3\n100/100 [==============================] - 35s 347ms/step - loss: 0.3629 - accuracy: 0.8583 - val_loss: 0.3980 - val_accuracy: 0.8314\nEpoch 3/3\n100/100 [==============================] - 35s 347ms/step - loss: 0.2923 - accuracy: 0.8922 - val_loss: 0.4382 - val_accuracy: 0.8258\n40/40 [==============================] - 4s 67ms/step\naccuracy:  0.8258471237194641\nf1 score:  0.7959372114496768\n102/102 [==============================] - 7s 66ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}